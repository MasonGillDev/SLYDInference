<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLYD LLM Portal</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <!-- Top Navigation Bar -->
    <nav class="topbar">
        <div class="nav-content">
            <div class="logo-section">
                <img src="https://www.slyd.com/images/SLYD-logo-300x86.webp" alt="SLYD Logo" class="logo">
            </div>
            <div class="nav-links">
                <a href="https://nm-vllm.readthedocs.io/en/0.4.0/serving/openai_compatible_server.html" target="_blank" class="docs-link">docs</a>
            </div>
        </div>
    </nav>

    <!-- Main Content -->
    <div class="container">


        <!-- HuggingFace Token Section -->
        <div class="card">
            <h2 class="section-title">HuggingFace Authentication</h2>
            <div class="form-group">
                <label for="hf-token">HuggingFace Token</label>
                <div class="token-display">
                    {% if masked_token %}
                        <span class="masked-token">{{ masked_token }}</span>
                        <button type="button" class="btn-secondary" onclick="editToken()">Edit</button>
                    {% else %}
                        <input type="password" id="hf-token" placeholder="hf_..." class="input-field">
                        <button type="button" class="btn-secondary" onclick="saveToken()">Save</button>
                    {% endif %}
                </div>
            </div>
        </div>

        <!-- Model Configuration Section -->
        <div class="card">
            <h2 class="section-title">Model Configuration</h2>

            <div class="form-group">
                <label for="model-id">HuggingFace Model ID</label>
                <div class="input-with-button">
                    <input type="text"
                           id="model-id"
                           value="{{ vllm_config.model }}"
                           placeholder="e.g., meta-llama/Llama-2-7b-hf"
                           class="input-field flex-1">
                    <button type="button" class="btn-check" onclick="checkModel()">Check</button>
                </div>
                <div id="model-status" class="status-message"></div>
            </div>

        </div>

        <!-- Performance Configuration Section -->
        <div class="card">
            <h2 class="section-title">Performance Settings</h2>

            <div class="form-row">
                <div class="form-group">
                    <label for="gpu-memory">GPU Memory Utilization</label>
                    <input type="number"
                           id="gpu-memory"
                           value="{{ vllm_config.gpu_memory_utilization }}"
                           step="0.05"
                           min="0.1"
                           max="1.0"
                           class="input-field">
                    <small class="helper-text">0.1 to 1.0 (default: 0.9)</small>
                </div>

                <div class="form-group">
                    <label for="max-model-len">Max Model Length</label>
                    <input type="number"
                           id="max-model-len"
                           value="{{ vllm_config.max_model_len }}"
                           class="input-field">
                    <small class="helper-text">Maximum context length</small>
                </div>
            </div>

            <div class="form-row">
                <div class="form-group">
                    <label for="tensor-parallel">Tensor Parallel Size</label>
                    <input type="number"
                           id="tensor-parallel"
                           value="{{ vllm_config.tensor_parallel_size }}"
                           min="1"
                           class="input-field">
                    <small class="helper-text">Number of GPUs to use</small>
                </div>

                <div class="form-group">
                    <label for="dtype">Data Type</label>
                    <select id="dtype" class="input-field">
                        <option value="auto" {% if vllm_config.dtype == 'auto' %}selected{% endif %}>auto</option>
                        <option value="float16" {% if vllm_config.dtype == 'float16' %}selected{% endif %}>float16</option>
                        <option value="bfloat16" {% if vllm_config.dtype == 'bfloat16' %}selected{% endif %}>bfloat16</option>
                        <option value="float32" {% if vllm_config.dtype == 'float32' %}selected{% endif %}>float32</option>
                    </select>
                </div>
            </div>

            <div class="form-row">
                <div class="form-group">
                    <label for="max-num-seqs">Max Number of Sequences</label>
                    <input type="number"
                           id="max-num-seqs"
                           value="{{ vllm_config.get('max_num_seqs', 256) }}"
                           min="1"
                           class="input-field">
                    <small class="helper-text">Max sequences per iteration (default: 256)</small>
                </div>

                <div class="form-group">
                    <label for="max-num-batched-tokens">Max Batched Tokens</label>
                    <input type="number"
                           id="max-num-batched-tokens"
                           value="{{ vllm_config.get('max_num_batched_tokens', '') }}"
                           placeholder="Auto"
                           class="input-field">
                    <small class="helper-text">Leave empty for auto</small>
                </div>
            </div>

            <div class="form-row">
                <div class="form-group">
                    <label for="quantization">Quantization Method</label>
                    <select id="quantization" class="input-field">
                        <option value="" {% if not vllm_config.get('quantization') %}selected{% endif %}>None</option>
                        <option value="awq" {% if vllm_config.get('quantization') == 'awq' %}selected{% endif %}>AWQ</option>
                        <option value="gptq" {% if vllm_config.get('quantization') == 'gptq' %}selected{% endif %}>GPTQ</option>
                        <option value="squeezellm" {% if vllm_config.get('quantization') == 'squeezellm' %}selected{% endif %}>SqueezeLLM</option>
                        <option value="fp8" {% if vllm_config.get('quantization') == 'fp8' %}selected{% endif %}>FP8</option>
                    </select>
                    <small class="helper-text">Weight quantization method</small>
                </div>

                <div class="form-group">
                    <label for="kv-cache-dtype">KV Cache Data Type</label>
                    <select id="kv-cache-dtype" class="input-field">
                        <option value="auto" {% if vllm_config.get('kv_cache_dtype', 'auto') == 'auto' %}selected{% endif %}>auto</option>
                        <option value="fp8" {% if vllm_config.get('kv_cache_dtype') == 'fp8' %}selected{% endif %}>fp8</option>
                        <option value="fp8_e5m2" {% if vllm_config.get('kv_cache_dtype') == 'fp8_e5m2' %}selected{% endif %}>fp8_e5m2</option>
                    </select>
                    <small class="helper-text">Data type for KV cache</small>
                </div>
            </div>

            <div class="form-group">
                <label class="checkbox-label">
                    <input type="checkbox"
                           id="enable-prefix-caching"
                           {% if vllm_config.get('enable_prefix_caching', False) %}checked{% endif %}>
                    <span>Enable Prefix Caching</span>
                </label>
                <small class="helper-text">Cache common prompt prefixes for faster inference</small>
            </div>

            <div class="form-group">
                <label class="checkbox-label">
                    <input type="checkbox"
                           id="enable-chunked-prefill"
                           {% if vllm_config.get('enable_chunked_prefill', False) %}checked{% endif %}>
                    <span>Enable Chunked Prefill</span>
                </label>
                <small class="helper-text">Process long prompts in chunks</small>
            </div>

            <div class="form-group">
                <label class="checkbox-label">
                    <input type="checkbox"
                           id="trust-remote-code"
                           {% if vllm_config.trust_remote_code %}checked{% endif %}>
                    <span>Trust Remote Code</span>
                </label>
                <small class="helper-text">Allow execution of custom model code from HuggingFace</small>
            </div>
        </div>

        <!-- Advanced Configuration Section -->
        <div class="card">
            <h2 class="section-title">Advanced Settings</h2>

            <div class="form-row">
                <div class="form-group">
                    <label for="tokenizer">Tokenizer (optional)</label>
                    <input type="text"
                           id="tokenizer"
                           value="{{ vllm_config.get('tokenizer', '') }}"
                           placeholder="Leave empty to use model's tokenizer"
                           class="input-field">
                    <small class="helper-text">Custom tokenizer path or name</small>
                </div>

                <div class="form-group">
                    <label for="revision">Model Revision (optional)</label>
                    <input type="text"
                           id="revision"
                           value="{{ vllm_config.get('revision', '') }}"
                           placeholder="main"
                           class="input-field">
                    <small class="helper-text">Git branch/tag (default: main)</small>
                </div>
            </div>

            <div class="form-group">
                <label for="download-dir">Download Directory (optional)</label>
                <input type="text"
                       id="download-dir"
                       value="{{ vllm_config.get('download_dir', '') }}"
                       placeholder="/path/to/model/cache"
                       class="input-field">
                <small class="helper-text">Where to download/cache model weights</small>
            </div>

            <div class="form-group">
                <label for="load-format">Load Format</label>
                <select id="load-format" class="input-field">
                    <option value="auto" {% if vllm_config.get('load_format', 'auto') == 'auto' %}selected{% endif %}>auto</option>
                    <option value="pt" {% if vllm_config.get('load_format') == 'pt' %}selected{% endif %}>PyTorch</option>
                    <option value="safetensors" {% if vllm_config.get('load_format') == 'safetensors' %}selected{% endif %}>SafeTensors</option>
                    <option value="npcache" {% if vllm_config.get('load_format') == 'npcache' %}selected{% endif %}>NPCache</option>
                </select>
                <small class="helper-text">Format of model weights to load</small>
            </div>
        </div>

        <!-- Action Buttons -->
        <div class="action-buttons">
            <button type="button" class="btn-primary" onclick="saveConfig()">Save Configuration</button>
            <button type="button" class="btn-secondary" onclick="resetToDefaults()">Reset to Defaults</button>
            <button type="button" class="btn-secondary" onclick="restartService()">Restart Service</button>
            <button type="button" class="btn-secondary" onclick="checkServiceStatus()">Check Status</button>
        </div>

        <!-- Toggle Raw Editor Button -->
        <div style="margin-top: 1.5rem; text-align: center;">
            <button type="button" class="btn-toggle" onclick="toggleRawEditor()">
                <span id="toggle-text">Show Raw JSON Editor</span>
            </button>
        </div>

        <!-- Raw JSON Editor (Hidden by default) -->
        <div id="raw-editor" class="raw-editor-container" style="display: none;">
            <div class="card">
                <h2 class="section-title">Raw Configuration Editor</h2>
                <p style="color: var(--text-secondary); font-size: 0.875rem; margin-bottom: 1rem;">
                    ⚠️ Advanced: Edit the raw JSON configuration. Make sure the JSON is valid before saving.
                </p>
                <textarea id="raw-config-textarea" class="raw-config-textarea" rows="20">{{ vllm_config | tojson(indent=2) }}</textarea>
                <div class="raw-editor-buttons">
                    <button type="button" class="btn-primary" onclick="saveRawConfig()">Save Raw Config</button>
                    <button type="button" class="btn-secondary" onclick="cancelRawEdit()">Cancel</button>
                </div>
                <div id="raw-editor-status" class="status-message"></div>
            </div>
        </div>

        <!-- Service Status -->
        <div id="service-status" class="status-card"></div>
    </div>

    <script>
        // Set API base URL for fetch requests
        window.API_BASE = "{{ request.script_root }}";
    </script>
    <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>
</html>
